{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Problem Statement/Overview\nIn this Kaggle competition, we use a set of small images showing microscopic tissue to train a model. This is therefore a classification problem: when the model sees future images of tissue, it should be trained in such a way that it can distinguish between tissues which present visual cues of metastatic cancer, and those which do not. Therefore, this is a binary problem.\n\nIn this specific dataset, we have over 277K images which are split between about 220K training images, with the rest being assigned to testing. There are no duplicate images in the dataset.\n\nImages are provided in .tif format, while a .csv file lists the names of the images (an assortment of alphanumeric characters) and the value of their classification, either a 1 or 0. We look deeper at this in the EDA section. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport zipfile\nimport os\nimport matplotlib.pyplot as plt\nimport shutil\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport io\n#from tensorflow.distribute import Strategy as strategy\nprint(\"TensorFlow version:\", tf.__version__)\nfrom tensorflow.keras import datasets, layers, models, optimizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom skimage.io import imread ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-17T03:16:52.136911Z","iopub.execute_input":"2022-10-17T03:16:52.137277Z","iopub.status.idle":"2022-10-17T03:16:59.009900Z","shell.execute_reply.started":"2022-10-17T03:16:52.137244Z","shell.execute_reply":"2022-10-17T03:16:59.008954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()","metadata":{"execution":{"iopub.status.busy":"2022-10-17T03:16:59.012775Z","iopub.execute_input":"2022-10-17T03:16:59.013705Z","iopub.status.idle":"2022-10-17T03:16:59.030166Z","shell.execute_reply.started":"2022-10-17T03:16:59.013665Z","shell.execute_reply":"2022-10-17T03:16:59.029559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = '/kaggle/input/histopathologic-cancer-detection/'","metadata":{"execution":{"iopub.status.busy":"2022-10-17T03:16:59.031667Z","iopub.execute_input":"2022-10-17T03:16:59.032252Z","iopub.status.idle":"2022-10-17T03:16:59.041775Z","shell.execute_reply.started":"2022-10-17T03:16:59.032209Z","shell.execute_reply":"2022-10-17T03:16:59.040784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = os.path.join(PATH, 'test/')\ntrain_path = os.path.join(PATH, 'train/')\ndf_training = pd.read_csv(PATH+'train_labels.csv')","metadata":{"execution":{"iopub.status.busy":"2022-10-17T03:16:59.044272Z","iopub.execute_input":"2022-10-17T03:16:59.044917Z","iopub.status.idle":"2022-10-17T03:16:59.560921Z","shell.execute_reply.started":"2022-10-17T03:16:59.044876Z","shell.execute_reply":"2022-10-17T03:16:59.559881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"First, let's take a look at the dataset using our .csv file of training images.","metadata":{}},{"cell_type":"code","source":"print(df_training.shape)\ndf_training.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-17T03:16:59.562159Z","iopub.execute_input":"2022-10-17T03:16:59.562521Z","iopub.status.idle":"2022-10-17T03:16:59.584752Z","shell.execute_reply.started":"2022-10-17T03:16:59.562480Z","shell.execute_reply":"2022-10-17T03:16:59.583838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see our training set is 220,025 images, with an ID and a label. These labels indicate whether the training photo shows signes of metastatic cancer or not, with a 1 indicating a positive result. Let's see how our dataset is split.","metadata":{}},{"cell_type":"code","source":"print(df_training.label.value_counts())\nzero, one = df_training.label.value_counts()\nprint(round((zero/len(df_training))*100,1),\"% images marked 0.\")\nprint(round((one/len(df_training))*100,1),\"% images marked 1.\")","metadata":{"execution":{"iopub.status.busy":"2022-10-17T03:01:30.473032Z","iopub.execute_input":"2022-10-17T03:01:30.473336Z","iopub.status.idle":"2022-10-17T03:01:30.490954Z","shell.execute_reply.started":"2022-10-17T03:01:30.473298Z","shell.execute_reply":"2022-10-17T03:01:30.490082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_training.label.value_counts().plot(kind = 'bar')","metadata":{"execution":{"iopub.status.busy":"2022-10-17T03:01:31.894914Z","iopub.execute_input":"2022-10-17T03:01:31.895382Z","iopub.status.idle":"2022-10-17T03:01:32.121637Z","shell.execute_reply.started":"2022-10-17T03:01:31.895347Z","shell.execute_reply":"2022-10-17T03:01:32.120553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we've seen what our training data breakdown looks like, let's see what an actual image looks like!","metadata":{}},{"cell_type":"code","source":"## These methods were adapted from the following medium post: https://vijayabhaskar96.medium.com/tutorial-on-keras-flow-from-dataframe-1fd4493d237c\n\ndef edit_filename(string):\n    return string+\".tif\"\n\ndf_training[\"id\"] = df_training[\"id\"].apply(edit_filename)\ndf_training['label'] = df_training['label'].astype(str)\n\ndf_training = shuffle(df_training, random_state=12)\n\nprint(df_training)","metadata":{"execution":{"iopub.status.busy":"2022-10-17T03:17:02.833839Z","iopub.execute_input":"2022-10-17T03:17:02.834221Z","iopub.status.idle":"2022-10-17T03:17:03.081350Z","shell.execute_reply.started":"2022-10-17T03:17:02.834185Z","shell.execute_reply":"2022-10-17T03:17:03.080277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen = ImageDataGenerator(rescale=1./255., validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-10-17T03:17:05.913841Z","iopub.execute_input":"2022-10-17T03:17:05.914543Z","iopub.status.idle":"2022-10-17T03:17:05.920005Z","shell.execute_reply.started":"2022-10-17T03:17:05.914498Z","shell.execute_reply":"2022-10-17T03:17:05.919128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    training_generator = datagen.flow_from_dataframe(dataframe=df_training, directory=train_path, x_col=\"id\", y_col=\"label\", subset=\"training\", batch_size=128, seed=12, class_mode=\"binary\", target_size=(64,64))","metadata":{"execution":{"iopub.status.busy":"2022-10-17T03:17:15.848047Z","iopub.execute_input":"2022-10-17T03:17:15.848615Z","iopub.status.idle":"2022-10-17T03:27:40.964893Z","shell.execute_reply.started":"2022-10-17T03:17:15.848583Z","shell.execute_reply":"2022-10-17T03:27:40.963804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    validation_generator = datagen.flow_from_dataframe(dataframe=df_training, directory=train_path, x_col=\"id\", y_col=\"label\", subset=\"validation\", batch_size=128, seed=12, class_mode=\"binary\", target_size=(64,64))  ","metadata":{"execution":{"iopub.status.busy":"2022-10-17T03:27:40.967054Z","iopub.execute_input":"2022-10-17T03:27:40.967495Z","iopub.status.idle":"2022-10-17T03:29:16.585426Z","shell.execute_reply.started":"2022-10-17T03:27:40.967454Z","shell.execute_reply":"2022-10-17T03:29:16.584334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_img = next(iter(training_generator))\nfor i in range(0, 4):\n    plt.imshow(sample_img[0][i])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-17T03:29:16.587038Z","iopub.execute_input":"2022-10-17T03:29:16.587748Z","iopub.status.idle":"2022-10-17T03:29:18.574040Z","shell.execute_reply.started":"2022-10-17T03:29:16.587694Z","shell.execute_reply":"2022-10-17T03:29:18.573005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Architecture\nDue to the nature of the assignment, I have chosen to use a Convolutional Neural Network (CNN) to classify our test images. Convolution is a useful tactic in dealing with images because, simply put, images have a lot of data (pixel height times pixel length times dimensions, oftentimes three). Because of this, a densely connected network will tend to run slow, and importantly, to overfit. Convolution networks try to overcome these shortcomings.\n\nBelow, I try out two models. Both are relatively simple, with the first model being more rudimentary.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    model = models.Sequential()\n    model.add(layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu'))\n    model.add(layers.AveragePooling2D(pool_size=(2,2)))\n    model.add(layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu'))\n  #  model.add(layers.MaxPooling2D(pool_size=(2,2)))\n    model.add(layers.AveragePooling2D(pool_size=(2,2)))\n    model.add(layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n    model.add(layers.AveragePooling2D(pool_size=(2,2)))\n  #  model.add(layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n #   model.add(layers.AveragePooling2D(pool_size=(2,2)))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(10, activation='sigmoid'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.build(input_shape=(256, 64, 64, 3)) ","metadata":{"execution":{"iopub.status.busy":"2022-10-17T03:37:07.395811Z","iopub.execute_input":"2022-10-17T03:37:07.396191Z","iopub.status.idle":"2022-10-17T03:37:07.461736Z","shell.execute_reply.started":"2022-10-17T03:37:07.396158Z","shell.execute_reply":"2022-10-17T03:37:07.460865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This first model uses several types of layers. There are four layers that are traditional 2-dimensional convolution layers, which have 16 layers apiece before the pooling layer and 32 filters after the pooling layer. THe kernel size across the board is 3x3, which is the height and width of the convolution window. The activation function specified here is 'relu', although other functions such as 'sigmoid' could also be applied. \n\nThe pooling layer is used to reduce the data by taking the average value in a given window. Here, we use a pool size of 2x2 in order to effectively sub-sample the input data. \n\nIn the end, we flatten the data using a flatten layer in order to feed the network into a dense layer, which we can append to the end of a CNN. In this case, we use a sigmoid activation function and we specify that the output should be 1, which is to say, a single dimension.\n\nFinally, in this first model, we are going to use a batch size of 256, which is relatively large, in order to loop through our data quickly to get a sense of starting values.","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-10-17T03:37:09.152943Z","iopub.execute_input":"2022-10-17T03:37:09.153616Z","iopub.status.idle":"2022-10-17T03:37:09.159964Z","shell.execute_reply.started":"2022-10-17T03:37:09.153580Z","shell.execute_reply":"2022-10-17T03:37:09.158776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model.compile(loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()], optimizer = optimizers.Adam(learning_rate=0.001))","metadata":{"execution":{"iopub.status.busy":"2022-10-17T03:37:23.858272Z","iopub.execute_input":"2022-10-17T03:37:23.858622Z","iopub.status.idle":"2022-10-17T03:37:23.873861Z","shell.execute_reply.started":"2022-10-17T03:37:23.858591Z","shell.execute_reply":"2022-10-17T03:37:23.872753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with strategy.scope():\nhistory = model.fit_generator(training_generator, epochs=60, validation_data = validation_generator)","metadata":{"execution":{"iopub.status.busy":"2022-10-17T03:42:52.773588Z","iopub.execute_input":"2022-10-17T03:42:52.774002Z","iopub.status.idle":"2022-10-17T04:05:47.032217Z","shell.execute_reply.started":"2022-10-17T03:42:52.773955Z","shell.execute_reply":"2022-10-17T04:05:47.020782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validate'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-17T04:05:49.909813Z","iopub.execute_input":"2022-10-17T04:05:49.910196Z","iopub.status.idle":"2022-10-17T04:05:49.934217Z","shell.execute_reply.started":"2022-10-17T04:05:49.910163Z","shell.execute_reply":"2022-10-17T04:05:49.932783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, after that first model, we're going to try a second model that has far more parameters, but try to also keep it somewhat reasonable given resource constraints.\n\nSimilar to the first model, we're going to have convolutional layers followed by pooling layers. Here, we're going to pair convolutional layers followed by a max pooling layer, a batch normalization layer, and a dropout layer. Both of these last two layer types can help with regularization, as we learned in lecture. We are going to have three sets of these layer types.\n\nWe will finish with an extra dense layer, again with a dropout layer to keep our parameter number under control. Finally, at the compile step, I'm using the Nadam optimizer and specifying the learning rate at 0.0001. Again, we're going to run for 60 epochs (since a previous version of a similar model did not converge over 40 epochs). \n\nHere we go!","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    training_generator = datagen.flow_from_dataframe(dataframe=df_training, directory=train_path, x_col=\"id\", y_col=\"label\", subset=\"training\", batch_size=256, seed=12, class_mode=\"binary\", target_size=(64,64))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    validation_generator = datagen.flow_from_dataframe(dataframe=df_training, directory=train_path, x_col=\"id\", y_col=\"label\", subset=\"validation\", batch_size=256, seed=12, class_mode=\"binary\", target_size=(64,64))  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model_v2 = models.Sequential()\n    model_v2.add(layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu'))\n    model_v2.add(layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu'))\n    model_v2.add(layers.MaxPooling2D(pool_size=(2,2)))\n    model_v2.add(layers.BatchNormalization())\n    model_v2.add(layers.Dropout(0.25))\n    \n    model_v2.add(layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n    model_v2.add(layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n    model_v2.add(layers.MaxPooling2D(pool_size=(2,2)))\n    model_v2.add(layers.BatchNormalization())\n    model_v2.add(layers.Dropout(0.25))\n    \n    model_v2.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n    model_v2.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n    model_v2.add(layers.MaxPooling2D(pool_size=(2,2)))\n    model_v2.add(layers.BatchNormalization())\n    model_v2.add(layers.Dropout(0.25))\n    \n    model_v2.add(layers.Flatten())\n    model_v2.add(layers.Dense(10, activation='sigmoid'))\n    model_v2.add(layers.Dropout(0.25))\n    model_v2.add(layers.Dense(1, activation='sigmoid'))\n    model_v2.build(input_shape=(256, 64, 64, 3)) ","metadata":{"execution":{"iopub.status.busy":"2022-10-17T04:05:55.162131Z","iopub.execute_input":"2022-10-17T04:05:55.162490Z","iopub.status.idle":"2022-10-17T04:05:55.275369Z","shell.execute_reply.started":"2022-10-17T04:05:55.162459Z","shell.execute_reply":"2022-10-17T04:05:55.274365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_v2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-10-17T04:05:57.044772Z","iopub.execute_input":"2022-10-17T04:05:57.045201Z","iopub.status.idle":"2022-10-17T04:05:57.056495Z","shell.execute_reply.started":"2022-10-17T04:05:57.045163Z","shell.execute_reply":"2022-10-17T04:05:57.055091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model_v2.compile(loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()], optimizer = optimizers.Nadam(learning_rate=0.0001))","metadata":{"execution":{"iopub.status.busy":"2022-10-17T04:08:23.149900Z","iopub.execute_input":"2022-10-17T04:08:23.150271Z","iopub.status.idle":"2022-10-17T04:08:23.168311Z","shell.execute_reply.started":"2022-10-17T04:08:23.150239Z","shell.execute_reply":"2022-10-17T04:08:23.167394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    history_v2 = model_v2.fit_generator(training_generator, epochs=60, validation_data = validation_generator)","metadata":{"execution":{"iopub.status.busy":"2022-10-17T04:08:26.850698Z","iopub.execute_input":"2022-10-17T04:08:26.851062Z","iopub.status.idle":"2022-10-17T04:16:27.467366Z","shell.execute_reply.started":"2022-10-17T04:08:26.851030Z","shell.execute_reply":"2022-10-17T04:16:27.465728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results and Analysis","metadata":{}},{"cell_type":"markdown","source":"Let's plot our results from this second model.","metadata":{}},{"cell_type":"code","source":"plt.plot(history_v2.history['accuracy'])\nplt.plot(history_v2.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validate'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T15:22:13.575052Z","iopub.execute_input":"2022-10-03T15:22:13.576106Z","iopub.status.idle":"2022-10-03T15:22:13.809016Z","shell.execute_reply.started":"2022-10-03T15:22:13.576058Z","shell.execute_reply":"2022-10-03T15:22:13.808069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history_v2.history['loss'])\nplt.plot(history_v2.history['val_loss'])\nplt.title('Loss by Epoch')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validate'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-03T15:22:19.616572Z","iopub.execute_input":"2022-10-03T15:22:19.617034Z","iopub.status.idle":"2022-10-03T15:22:19.827081Z","shell.execute_reply.started":"2022-10-03T15:22:19.617001Z","shell.execute_reply":"2022-10-03T15:22:19.826123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above, we can see that our more expensive model doesn't actually perform that much better, converges at about the same rate, and the validation accuracy is much more sporatic than our baseline model.\n\nIs this second model better than the first? In some ways, it appears that some of our \"extra\" parameters are not as helpful as they may appear. Even though the training accuracy curves appear similar in both of these models, the validation accuracy curve (and therefore the validation loss curve) for the second has much more variation involved, meaning that we are likely overfitting with this model. Regardless, we will use this model to score on the Kaggle leaderboard.\n\nNow that we've trained our two models, let's generate the data for our testing images and build a submission document based on this data. You will find the results of this submission in part III of this assignment submission within Coursera.","metadata":{}},{"cell_type":"code","source":"test_images = pd.DataFrame({'id':os.listdir(test_path)})\ntesting_datagen = ImageDataGenerator(rescale=1./255.)\ntesting_generator = testing_datagen.flow_from_dataframe(dataframe=test_images, directory=test_path, x_col='id', y_col=None, target_size=(64,64) ,batch_size=1, shuffle=False, class_mode=None)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T15:22:26.760498Z","iopub.execute_input":"2022-10-03T15:22:26.760850Z","iopub.status.idle":"2022-10-03T15:24:33.329062Z","shell.execute_reply.started":"2022-10-03T15:22:26.760820Z","shell.execute_reply":"2022-10-03T15:24:33.327998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict = model_v2.predict(testing_generator, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T15:26:54.197747Z","iopub.execute_input":"2022-10-03T15:26:54.198247Z","iopub.status.idle":"2022-10-03T15:34:56.902389Z","shell.execute_reply.started":"2022-10-03T15:26:54.198206Z","shell.execute_reply":"2022-10-03T15:34:56.901361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.DataFrame()\nsubmit['id'] = test_images['id'].apply(lambda x: x.split('.')[0])\nsubmit['label'] = list(map(lambda x: 0 if x < 0.5 else 1, predict))\nsubmit.head()\nsubmit.to_csv('cnn_submit.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T15:37:07.928385Z","iopub.execute_input":"2022-10-03T15:37:07.928763Z","iopub.status.idle":"2022-10-03T15:37:08.251989Z","shell.execute_reply.started":"2022-10-03T15:37:07.928732Z","shell.execute_reply":"2022-10-03T15:37:08.250765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\nAfter working with CNNs and trying multiple options, it's easy to feel like your head is spinning. Between selecting layer types, hyperparameters within each layer, activation and optimization techniques, there are a lot of factors to consider.\n\nIn addition to this, given the vast computing resources available now, it can be easy to add far more layers and utilize many more techniques than are necessary. \n\nMy largest takeaway is related to this: with all of the factors that one can control when creating a model and because you can maximize your accuracy to the training (and even to the validation) set to an incredible degree, it is easy to have a lot of confidence that your model is very accurate and strong. However, I think it's important, especially when dealing with medical imaging and potentially very impactful and life-altering data such as this, to keep in mind that no model is flawless, and design should be taken very seriously. While most of the work above was the result of trial and error, reading blogs, and referencing documentation, seeking guidance from qualified experts is an important step in using deep learning.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}